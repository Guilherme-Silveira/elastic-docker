input {
	file {
		path => "/usr/share/logstash/pipeline/teste/streamsets.json"
		sincedb_path => "/dev/null"
		start_position => "beginning"
		mode => "read"
	}
}
## Add your filters / logstash plugins configuration here
filter {

#Transform the string message in a JSON file
#If you already receive a JSON from the Streamsets API, ignore this filter
	json {
		source => "message"
		target => "temp"
		remove_field => ["message", "event"]
	}

#Transform the content of the JSON in an array
#Replace the event.get("temp") for the name of the field where the Streamsets API is stored, e.g. event.get("message")
	ruby { 
		code => '
			a = []
			temp = event.get("temp")
			temp.each { |k,v|
				v["pipeline"] = k
				a << v
			}
			event.set("temp_array", a)
		'
		remove_field => ["temp"]
	}

#Split the array in different events
	split {
		field => "temp_array"
		target => "streamsets"
		remove_field => ["temp_array"]
	}

#Optional - Let the fields of Streamsets on the root level of the document
	ruby {
		code => '
				event.get("streamsets").each { |k, v|
						event.set(k,v)
				}
				event.remove("streamsets")
		'
	}

}
output {
	elasticsearch {
		hosts => ["http://elasticsearch:9200"]
		index => "test"
		user => "elastic"
		password => "gui@123"
	}
}
